{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c93026a-ff9d-4229-bd0e-b0149acb3805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19445fdd-5715-4f63-ad6a-3da66d421088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "training_file_raw = 'training_set_VU_DM.csv'\n",
    "test_file_raw = 'test_set_VU_DM.csv'\n",
    "training_file_stats = 'training_set_stats_VU_DM.csv'\n",
    "test_file_stats = 'test_set_stats_VU_DM.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8469dfe-8dd8-4825-8fef-ae02f1282e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 54\n",
      "Number of observations: 2476054\n",
      "Number of rows with missing values: 2476054\n",
      "Number of columns with missing values: 51\n",
      "Percentage not-missing data for features with missing values:\n",
      "visitor_location_country_id: 100.00% not missing\n",
      "visitor_hist_starrating: 5.06% not missing\n",
      "visitor_hist_adr_usd: 5.07% not missing\n",
      "prop_country_id: 100.00% not missing\n",
      "prop_id: 100.00% not missing\n",
      "prop_starrating: 100.00% not missing\n",
      "prop_review_score: 99.85% not missing\n",
      "prop_brand_bool: 100.00% not missing\n",
      "prop_location_score1: 100.00% not missing\n",
      "prop_location_score2: 78.14% not missing\n",
      "prop_log_historical_price: 100.00% not missing\n",
      "position: 100.00% not missing\n",
      "price_usd: 100.00% not missing\n",
      "promotion_flag: 100.00% not missing\n",
      "srch_destination_id: 100.00% not missing\n",
      "srch_length_of_stay: 100.00% not missing\n",
      "srch_booking_window: 100.00% not missing\n",
      "srch_adults_count: 100.00% not missing\n",
      "srch_children_count: 100.00% not missing\n",
      "srch_room_count: 100.00% not missing\n",
      "srch_saturday_night_bool: 100.00% not missing\n",
      "srch_query_affinity_score: 6.43% not missing\n",
      "orig_destination_distance: 67.69% not missing\n",
      "random_bool: 100.00% not missing\n",
      "comp1_rate: 2.39% not missing\n",
      "comp1_inv: 2.58% not missing\n",
      "comp1_rate_percent_diff: 1.89% not missing\n",
      "comp2_rate: 40.90% not missing\n",
      "comp2_inv: 43.03% not missing\n",
      "comp2_rate_percent_diff: 11.20% not missing\n",
      "comp3_rate: 31.07% not missing\n",
      "comp3_inv: 33.43% not missing\n",
      "comp3_rate_percent_diff: 9.57% not missing\n",
      "comp4_rate: 6.13% not missing\n",
      "comp4_inv: 6.86% not missing\n",
      "comp4_rate_percent_diff: 2.61% not missing\n",
      "comp5_rate: 44.70% not missing\n",
      "comp5_inv: 47.47% not missing\n",
      "comp5_rate_percent_diff: 16.91% not missing\n",
      "comp6_rate: 4.83% not missing\n",
      "comp6_inv: 5.25% not missing\n",
      "comp6_rate_percent_diff: 1.94% not missing\n",
      "comp7_rate: 6.33% not missing\n",
      "comp7_inv: 7.15% not missing\n",
      "comp7_rate_percent_diff: 2.79% not missing\n",
      "comp8_rate: 38.79% not missing\n",
      "comp8_inv: 40.21% not missing\n",
      "comp8_rate_percent_diff: 12.39% not missing\n",
      "click_bool: 100.00% not missing\n",
      "gross_bookings_usd: 2.79% not missing\n",
      "booking_bool: 100.00% not missing\n"
     ]
    }
   ],
   "source": [
    "def dataset_stats(df):\n",
    "    print(f'Number of features: {len(df.columns)}')\n",
    "    total_observations = len(df)\n",
    "    print(f'Number of observations: {len(df)}')\n",
    "    print(f'Number of rows with missing values: {df.isnull().any(axis=1).sum()}')\n",
    "    print(f'Number of columns with missing values: {df.isnull().any(axis=0).sum()}')\n",
    "    print(f'Percentage not-missing data for features with missing values:')\n",
    "    for feature in df.columns[df.isnull().any()]:\n",
    "        print(f\"{feature}: {100*(total_observations - df[feature].isnull().sum())/total_observations:.2f}% not missing\")\n",
    "dataset_stats(load_dataset(training_file_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "954d6918-15aa-4817-85a9-13dd564588ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_dataset(training_file_raw)\n",
    "df_test = load_dataset(test_file_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbeacc65-8cb1-4527-ac1d-4071873c4ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(min(df_train['prop_starrating']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92d6acd2-33c7-4cc1-8631-647ccb01db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_engineered_columns(df_raw):\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    df.loc[df['price_usd'] > 2060.0355, 'price_usd'] = np.nan # 0.999 percent of data maar kunnen dit nog aanpassen\n",
    "    df['price_per_night'] = df['price_usd'] / df['srch_length_of_stay']\n",
    "    df['month'] = pd.to_datetime(df['date_time']).dt.month\n",
    "\n",
    "    df['review_score_relative'] = (df['prop_review_score'] - df.groupby('srch_id')['prop_review_score'].transform('median'))\n",
    "    df['price_relative'] = (df['price_usd'] - df.groupby('srch_id')['price_usd'].transform('median'))\n",
    "\n",
    "    df['log_price_usd'] = np.log1p(df['price_usd'])\n",
    "    df['log_price_per_night'] = np.log1p(df['price_per_night'])\n",
    "\n",
    "    df['orig_dest_missing'] = df['orig_destination_distance'].isna().astype(int)\n",
    "    df['loc_score2_missing'] = df['prop_location_score2'].isna().astype(int)\n",
    "\n",
    "    #for col in ['orig_destination_distance', 'prop_location_score2']:\n",
    "    #    if col in df.columns:\n",
    "    #        df[col] = df[col].fillna(df[col].median())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f52b251a-6852-42a5-948b-91451cf19900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prop_stats(df):\n",
    "    exclude = [\n",
    "        'srch_id', 'prop_id', 'position', 'click_bool', 'booking_bool',\n",
    "        'gross_bookings_usd', 'relevance', 'price_usd', 'visitor_location_country_id',\n",
    "        'prop_country_id', 'site_id', 'srch_destination_id'\n",
    "    ]\n",
    "    numeric = df.select_dtypes('number').columns.drop(exclude)\n",
    "    means = df.groupby('prop_id')[numeric].mean().add_suffix('_mean')\n",
    "    meds  = df.groupby('prop_id')[numeric].median().add_suffix('_median')\n",
    "    stats = means.join(meds)\n",
    "    return stats\n",
    "\n",
    "def prepare_final_features(df, prop_stats):\n",
    "    df = df.merge(prop_stats, on='prop_id', how='left')\n",
    "    return df.drop(['position', 'click_bool', 'booking_bool', 'gross_bookings_usd', 'price_usd', 'date_time'], axis=1, errors='ignore') #ignores de waardes als er geen column is om te droppen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4af1e61e-de8c-41f6-86b2-b05c976267f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(\"training_set_VU_DM.csv\")\n",
    "test_raw  = pd.read_csv(\"test_set_VU_DM.csv\")\n",
    "\n",
    "train_fe = add_engineered_columns(train_raw)\n",
    "test_fe  = add_engineered_columns(test_raw)\n",
    "\n",
    "train_fe['relevance'] = 0\n",
    "train_fe.loc[train_fe['click_bool'] == 1, 'relevance'] = 1\n",
    "train_fe.loc[train_fe['booking_bool'] == 1, 'relevance'] = 5\n",
    "\n",
    "prop_stats = compute_prop_stats(train_fe)\n",
    "\n",
    "train_final = prepare_final_features(train_fe, prop_stats)\n",
    "test_final  = prepare_final_features(test_fe, prop_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3863c055-8fc8-417b-be1b-75a9de67ca43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.2398205\tbest: 0.2398205 (0)\ttotal: 3.24s\tremaining: 53m 56s\n",
      "100:\ttest: 0.3783813\tbest: 0.3783813 (100)\ttotal: 4m 50s\tremaining: 43m 6s\n",
      "200:\ttest: 0.3865961\tbest: 0.3865961 (200)\ttotal: 8m 45s\tremaining: 34m 49s\n",
      "300:\ttest: 0.3913125\tbest: 0.3914641 (298)\ttotal: 12m 32s\tremaining: 29m 8s\n",
      "400:\ttest: 0.3939231\tbest: 0.3943669 (396)\ttotal: 16m 9s\tremaining: 24m 8s\n",
      "500:\ttest: 0.3947529\tbest: 0.3951660 (486)\ttotal: 19m 22s\tremaining: 19m 18s\n",
      "600:\ttest: 0.3965940\tbest: 0.3965940 (600)\ttotal: 22m 35s\tremaining: 14m 59s\n",
      "700:\ttest: 0.3959902\tbest: 0.3966720 (624)\ttotal: 25m 42s\tremaining: 10m 58s\n",
      "800:\ttest: 0.3967029\tbest: 0.3968585 (793)\ttotal: 28m 52s\tremaining: 7m 10s\n",
      "900:\ttest: 0.3982731\tbest: 0.3982731 (900)\ttotal: 32m 1s\tremaining: 3m 31s\n",
      "999:\ttest: 0.3997913\tbest: 0.3999148 (997)\ttotal: 35m 7s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.3999148091\n",
      "bestIteration = 997\n",
      "\n",
      "Shrink model to first 998 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRanker at 0x11a9d0910>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostRanker, Pool\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# features + labels\n",
    "X = train_final.drop(['srch_id', 'relevance'], axis=1)\n",
    "y = train_final['relevance']\n",
    "\n",
    "# grouping srch_id\n",
    "groups = train_final.groupby('srch_id').size().to_numpy()\n",
    "\n",
    "# splitting on srch_id\n",
    "gss = GroupShuffleSplit(test_size=0.2, random_state=1)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=train_final['srch_id']))\n",
    "\n",
    "X_train_all = X.iloc[train_idx]\n",
    "y_train_all = y.iloc[train_idx]\n",
    "groups_train_all = train_final.iloc[train_idx]['srch_id']\n",
    "\n",
    "# get val set out of training\n",
    "gss_val = GroupShuffleSplit(test_size=0.2, random_state=2)\n",
    "train_idx_final, val_idx = next(gss_val.split(X_train_all, y_train_all, groups=groups_train_all))\n",
    "\n",
    "X_train = X_train_all.iloc[train_idx_final]\n",
    "y_train = y_train_all.iloc[train_idx_final]\n",
    "X_val = X_train_all.iloc[val_idx]\n",
    "y_val = y_train_all.iloc[val_idx]\n",
    "\n",
    "# group IDs\n",
    "group_id_train = groups_train_all.iloc[train_idx_final].values\n",
    "group_id_val = groups_train_all.iloc[val_idx].values\n",
    "\n",
    "# making pools\n",
    "train_pool = Pool(data=X_train, label=y_train, group_id=group_id_train)\n",
    "val_pool = Pool(data=X_val, label=y_val, group_id=group_id_val)\n",
    "\n",
    "# catboost model\n",
    "model = CatBoostRanker(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    loss_function='YetiRank',\n",
    "    eval_metric='NDCG:top=5',\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# train the model\n",
    "model.fit(train_pool, eval_set=val_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63af0c36-731c-4185-941d-76b0326b2fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file done\n"
     ]
    }
   ],
   "source": [
    "feature_names = model.feature_names_\n",
    "X_submit = test_final[feature_names]\n",
    "\n",
    "catboost_preds = model.predict(X_submit)\n",
    "test_final['pred'] = catboost_preds\n",
    "\n",
    "test_df_filtered = test_final[['srch_id', 'prop_id', 'pred']]\n",
    "\n",
    "test_df_sorted = test_df_filtered.sort_values(by=['srch_id', 'pred'], ascending=[True, False])\n",
    "\n",
    "df_submission = test_df_sorted.drop(columns=['pred'])\n",
    "df_submission.to_csv('VU-DM-2025-Group-23.csv', index=False)\n",
    "print(\"file done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "317aa6bd-4a0b-4081-b251-c48cec9403da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Training model 1/8 met params: {'depth': 6, 'l2_leaf_reg': 1, 'random_strength': 1}\n",
      "0:\ttest: 0.2346201\tbest: 0.2346201 (0)\ttotal: 2.69s\tremaining: 44m 42s\n",
      "100:\ttest: 0.3781072\tbest: 0.3782044 (99)\ttotal: 3m 43s\tremaining: 33m 6s\n",
      "200:\ttest: 0.3868314\tbest: 0.3868314 (200)\ttotal: 6m 42s\tremaining: 26m 41s\n",
      "300:\ttest: 0.3920654\tbest: 0.3920654 (300)\ttotal: 9m 43s\tremaining: 22m 35s\n",
      "400:\ttest: 0.3944636\tbest: 0.3946296 (394)\ttotal: 12m 40s\tremaining: 18m 55s\n",
      "500:\ttest: 0.3968989\tbest: 0.3969003 (499)\ttotal: 15m 35s\tremaining: 15m 31s\n",
      "600:\ttest: 0.3968503\tbest: 0.3974550 (562)\ttotal: 18m 26s\tremaining: 12m 14s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.3974549705\n",
      "bestIteration = 562\n",
      "\n",
      "Shrink model to first 563 iterations.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 43\u001b[0m\n\u001b[1;32m     35\u001b[0m model \u001b[38;5;241m=\u001b[39m CatBoostRanker(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m     37\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     38\u001b[0m     train_pool,\n\u001b[1;32m     39\u001b[0m     eval_set\u001b[38;5;241m=\u001b[39mval_pool,\n\u001b[1;32m     40\u001b[0m     use_best_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     41\u001b[0m )\n\u001b[0;32m---> 43\u001b[0m best_ndcg \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDCG:top=5\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     45\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcombo,\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid_ndcg@5\u001b[39m\u001b[38;5;124m\"\u001b[39m: best_ndcg,\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_iteration\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39mget_best_iteration()\n\u001b[1;32m     49\u001b[0m })\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model  \u001b[38;5;66;03m# opschonen\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'validation'"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "base_params = {\n",
    "    \"iterations\": 1000,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"loss_function\": \"YetiRank\",\n",
    "    \"eval_metric\": \"NDCG:top=5\",\n",
    "    \"verbose\": 100,\n",
    "    \"random_seed\": 42,\n",
    "    \"task_type\": \"CPU\", \n",
    "    \"early_stopping_rounds\": 50\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    \"depth\": [6, 8],\n",
    "    \"l2_leaf_reg\": [1, 5],\n",
    "    \"random_strength\": [1, 5]\n",
    "}\n",
    "\n",
    "keys, values = zip(*param_grid.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, combo in enumerate(param_combinations):\n",
    "    print(f\"\\n Training model {i+1}/{len(param_combinations)} met params: {combo}\")\n",
    "    params = {**base_params, **combo}\n",
    "\n",
    "    model = CatBoostRanker(**params)\n",
    "\n",
    "    model.fit(\n",
    "        train_pool,\n",
    "        eval_set=val_pool,\n",
    "        use_best_model=True\n",
    "    )\n",
    "\n",
    "    best_ndcg = model.get_best_score()[\"validation\"][\"NDCG:top=5\"]\n",
    "\n",
    "    results.append({\n",
    "        **combo,\n",
    "        \"valid_ndcg@5\": best_ndcg,\n",
    "        \"best_iteration\": model.get_best_iteration()\n",
    "    })\n",
    "\n",
    "    del model \n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by=\"valid_ndcg@5\", ascending=False)\n",
    "results_df.to_csv(\"catboost_grid_search_results.csv\", index=False)\n",
    "\n",
    "print(\"\\n Beste resultaten:\")\n",
    "print(results_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
